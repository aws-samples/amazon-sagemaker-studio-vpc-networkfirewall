{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting world temperature with DeepAR\n",
    "- [Source](https://julsimon.medium.com/predicting-world-temperature-with-time-series-and-deepar-on-amazon-sagemaker-e371cf94ddb5)  \n",
    "- [Dataset - Daily Land](http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.70.0\n"
     ]
    }
   ],
   "source": [
    "# import data science and visualization libraries\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sagemaker\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker import image_uris\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-12 21:05:10--  http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt\n",
      "Resolving berkeleyearth.lbl.gov (berkeleyearth.lbl.gov)... 128.3.29.26\n",
      "Connecting to berkeleyearth.lbl.gov (berkeleyearth.lbl.gov)|128.3.29.26|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2737067 (2.6M) [text/plain]\n",
      "Saving to: ‘./data/Complete_TAVG_daily.txt.1’\n",
      "\n",
      "Complete_TAVG_daily 100%[===================>]   2.61M  4.25MB/s    in 0.6s    \n",
      "\n",
      "2022-01-12 21:05:10 (4.25 MB/s) - ‘./data/Complete_TAVG_daily.txt.1’ saved [2737067/2737067]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ./data/ http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1880.001     1880     1     1        1      -0.815\n",
      "  1880.004     1880     1     2        2       0.538\n",
      "  1880.007     1880     1     3        3       0.157\n",
      "  1880.010     1880     1     4        4       1.243\n",
      "  1880.012     1880     1     5        5       0.396\n",
      "  1880.015     1880     1     6        6       0.400\n",
      "  1880.018     1880     1     7        7       0.547\n",
      "  1880.021     1880     1     8        8       0.565\n",
      "  1880.023     1880     1     9        9       0.338\n",
      "  1880.026     1880     1    10       10       0.048\n"
     ]
    }
   ],
   "source": [
    "# Remove header lines (starting with a %), empty lines and lines with only spaces\n",
    "!grep -v -e '^%\\|^$\\|^\\ *$' ./data/Complete_TAVG_daily.txt > ./data/temps.txt\n",
    "!head -10 ./data/temps.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "minYear  = 1880\n",
    "maxYear  = 2021\n",
    "avg_temp = 8.68\n",
    "\n",
    "# Our model will predict temperature for the next 'prediction_length' days\n",
    "prediction_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/temps.txt', 'r')\n",
    "data = csv.reader(f,delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset={}\n",
    "x=[]\n",
    "y=[]\n",
    "count=1\n",
    "prevYear=0\n",
    "\n",
    "for row in data:\n",
    "        # Remove empty strings caused by multiple spaces between columns\n",
    "        row = list(filter(None, row))\n",
    "        \n",
    "        year=row[1]\n",
    "        temp=float(row[5])+avg_temp\n",
    "         \n",
    "        # Data for plotting\n",
    "        # x list=counter, y list=temperature\n",
    "        x.append(count)\n",
    "        y.append(float(temp))\n",
    "        count += 1\n",
    "        \n",
    "        # Data for training\n",
    "        # dictionary: key=year, value=list of ordered daily temperatures\n",
    "        if (year != prevYear):\n",
    "            dataset[year]=[]\n",
    "            prevYear=year\n",
    "        dataset[year].append(float(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes 'pythonic' rhymes with 'moronic' :D\n",
    "nb_samples_per_year = list(map(lambda x: len(x), (dataset[str(year)] for year in range(minYear, maxYear+1))))\n",
    "nb_samples_per_year = np.unique(nb_samples_per_year).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 365, 366]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nb_samples_per_year == [128, 365, 366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1d5670d7718a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnbSamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of samples: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnbSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "nbSamples=len(x)\n",
    "print('Number of samples: %d' % nbSamples)\n",
    "\n",
    "fig=plt.figure(figsize=(64, 16))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = dataset.copy()\n",
    "trainingSet[year] = { year: dataset[year][:-prediction_length] for year in dataset.keys() }\n",
    "testSet = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key      = 'deepar_training.json'\n",
    "test_key       = 'deepar_test.json'\n",
    "\n",
    "def writeDataset(filename, data): \n",
    "    file=open(filename,'w')\n",
    "    for year in data.keys():\n",
    "        # One JSON sample per line\n",
    "        line = \"\\\"start\\\":\\\"{}-01-01 00:00:00\\\",\\\"target\\\":{}\".format(year,data[year])\n",
    "        file.write('{'+line+'}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDataset(train_key, trainingSet)        \n",
    "writeDataset(test_key, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 deepar_training.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"deepar-daily-temperature\"\n",
    "\n",
    "train_prefix   = f'{prefix}/train'\n",
    "test_prefix    = f'{prefix}/test'\n",
    "output_prefix  = f'{prefix}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-906545278380/deepar-daily-temperature/train/deepar_training.json\n",
      "s3://sagemaker-us-east-1-906545278380/deepar-daily-temperature/test/deepar_test.json\n",
      "s3://bucket/output_prefix\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role              = sagemaker.get_execution_role()\n",
    "region            = boto3.Session().region_name\n",
    "\n",
    "train_path  = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)\n",
    "test_path   = sagemaker_session.upload_data(test_key,  bucket=bucket, key_prefix=test_prefix)\n",
    "output_path = f's3://bucket/output_prefix'\n",
    "\n",
    "print(train_path)\n",
    "print(test_path)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-12 21:24:14     493808 deepar-daily-temperature/test/deepar_test.json\n",
      "2022-01-12 21:24:14     940239 deepar-daily-temperature/train/deepar_training.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{bucket}/{prefix} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "container = image_uris.retrieve(framework='forecasting-deepar',region=region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.8xlarge',\n",
    "    base_job_name='daily-temperature',\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": 'D', # daily series\n",
    "    \"context_length\": prediction_length,\n",
    "    \"prediction_length\": prediction_length, # number of data points to predict\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"250\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.00001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\" # stop if loss hasn't improved in 10 epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
